<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://elfsong.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://elfsong.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-15T16:56:52+00:00</updated><id>https://elfsong.github.io/feed.xml</id><title type="html">blank</title><subtitle>The Personal Website of Du Mingzhe (杜明哲). </subtitle><entry><title type="html">Preference Alignment 101</title><link href="https://elfsong.github.io/blog/2026/preference_alignment/" rel="alternate" type="text/html" title="Preference Alignment 101"/><published>2026-01-14T00:00:00+00:00</published><updated>2026-01-14T00:00:00+00:00</updated><id>https://elfsong.github.io/blog/2026/preference_alignment</id><content type="html" xml:base="https://elfsong.github.io/blog/2026/preference_alignment/"><![CDATA[<h3 id="chapter-i-imitation-learning">Chapter I: Imitation Learning</h3> <h4 id="supervised-fine-tuning-sft">Supervised Fine-Tuning (SFT)</h4> <p>This is the first step in the alignment pipeline, transitioning from “Next Token Prediction” (Pre-training) to “Instruction Following”. It bridges the gap between <em>the vast knowledge base of the model</em> and <em>the user’s intent</em>.</p> \[\mathcal{L}_{\text{SFT}} = - \mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \sum_{t=1}^{T} \log \pi_\theta (y_t | x, y_{&lt;t}) \right]\] <ul> <li><strong>Pros:</strong> <code class="language-plaintext highlighter-rouge">simple implementation</code> (standard cross-entropy loss), <code class="language-plaintext highlighter-rouge">stable convergence</code>.</li> <li><strong>Cons:</strong> <code class="language-plaintext highlighter-rouge">exposure bias</code> (training on ground truth, testing on self-generated output) and <code class="language-plaintext highlighter-rouge">lack of negative feedback</code> (the model learns <em>what to do</em>, but not necessarily <em>what not to do</em>). It mimics the dataset distribution rather than optimizing for response quality.</li> </ul> <h3 id="chapter-ii-reinforcement-learning-from-human-feedback-rlhf">Chapter II: Reinforcement Learning from Human Feedback (RLHF)</h3> <p>SFT struggles to discern “better” from “good”. Humans are often better at judging quality than writing perfect demonstrations. RLHF introduces a <strong>Reward Model</strong> to act as a proxy for human preference, allowing the model to explore and optimize for higher rewards.</p> <h4 id="reinforce">REINFORCE</h4> <p>REINFORCE is the fundamental <em>Monte Carlo Policy Gradient</em> algorithm. It updates the policy by estimating the gradient using full response trajectories. Simply put: <em>if a generated sequence gets a high reward, the model increases the probability of all tokens in that sequence; if it gets a low reward, it decreases them.</em></p> \[\mathcal{L}_{\text{REINFORCE}} = - \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta (\cdot|x)} \left[ r(x, y) \cdot \sum_{t=1}^{T} \log \pi_\theta(y_t | x, y_{&lt;t}) \right]\] <ul> <li> <p><strong>Pros:</strong> theoretically <code class="language-plaintext highlighter-rouge">straightforward</code> and <code class="language-plaintext highlighter-rouge">unbiased</code> (asymptotically) implementation of the policy gradient theorem.</p> </li> <li> <p><strong>Cons:</strong> because it relies on full Monte Carlo returns, the gradient estimates are <code class="language-plaintext highlighter-rouge">extremely noisy</code>, making <code class="language-plaintext highlighter-rouge">training unstable</code>. It also requires a large number of samples to converge. Without a mechanism to limit update size, a single bad update can <code class="language-plaintext highlighter-rouge">ruin the policy</code>.</p> </li> </ul> <h4 id="proximal-policy-optimization-ppo">Proximal Policy Optimization (PPO)</h4> <p>PPO improves upon REINFORCE by adopting an <em>Actor-Critic</em> architecture and a <em>Trust Region</em> approach. It introduces a <em>Critic</em> (Value Function) to reduce variance and a <em>Clipping</em> mechanism to constrain the policy update. This ensures the new policy $\pi_\theta$ does not deviate too drastically away from the old policy $\pi_{\text{old}}$ in a single step.</p> \[\mathcal{L}_{\text{PPO}} = - \mathbb{E}_{(x,y)} \left[ \min \left( \rho_t(\theta) A_t, \text{clip}(\rho_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right] - \beta_{\text{KL}} D_{\text{KL}}(\pi_\theta || \pi_{\text{ref}})\] \[\rho_t(\theta) = \frac{\pi_\theta(y_t|x)}{\pi_{\text{old}}(y_t|x)}\] <ul> <li> <p><strong>Pros:</strong> the clipping mechanism <code class="language-plaintext highlighter-rouge">prevents catastrophic forgetting</code> and ensures <code class="language-plaintext highlighter-rouge">reliable convergence</code>.</p> </li> <li> <p><strong>Cons:</strong> a standard PPO setup requires loading four models into memory simultaneously: the Actor (Policy), Critic (Value), Reference Model, and Reward Model. This creates a <code class="language-plaintext highlighter-rouge">massive resource bottleneck</code> and makes hyperparameter tuning notoriously difficult.</p> </li> </ul> <h3 id="chapter-iii-direct-alignment">Chapter III: Direct Alignment</h3> <p>Researchers asked: “If the optimal policy $\pi^*$ can be expressed in terms of the reward and reference, can we optimize the policy directly <code class="language-plaintext highlighter-rouge">without training a separate Reward Model</code>?” This led to the era of Direct Alignment, which implicitly solves the RL problem using supervised objectives.</p> <h4 id="the-starting-point-rlhf-objective">The Starting Point: RLHF Objective</h4> <p>We start with the standard objective function for Reinforcement Learning from Human Feedback (RLHF). We want to maximize the expected reward $r(x,y)$ while penalizing the model if it drifts too far from the reference model $\pi_{ref}$ (using KL divergence).</p> \[\max_{\pi} J(\pi) = \mathbb{E}_{y \sim \pi(\cdot|x)} \left[ r(x, y) \right] - \beta D_{KL}(\pi(\cdot|x) || \pi_{ref}(\cdot|x))\] <h5 id="1-algebraic-expansion">1. Algebraic Expansion</h5> <p>we expand the KL divergence term using logarithm properties ($\log \frac{a}{b} = \log a - \log b$) to separate the policy and reference terms.</p> \[J(\pi) = \sum_y \pi(y|x) r(x, y) - \beta \sum_y \pi(y|x) \log \frac{\pi(y|x)}{\pi_{ref}(y|x)}\] \[J(\pi) = \sum_y \pi(y|x) \left( r(x, y) - \beta \log \frac{\pi(y|x)}{\pi_{ref}(y|x)} \right)\] \[J(\pi) = \sum_y \pi(y|x) \left( r(x, y) - \beta \log \pi(y|x) + \beta \log \pi_{ref}(y|x) \right)\] <h5 id="2-factorization">2. Factorization</h5> <p>we factor out $-\beta$ to reorganize the terms. This is a mathematical trick to make the equation look like a new KL divergence formula.</p> \[J(\pi) = -\beta \sum_y \pi(y|x) \left( \log \pi(y|x) - \log \pi_{ref}(y|x) - \frac{1}{\beta} r(x, y) \right)\] <h5 id="3-defining-the-optimal-policy">3. Defining the Optimal Policy</h5> <p>we define a theoretical optimal policy $\pi^<em>$ (closed-form solution) that follows a *Boltzmann distribution</em>. This represents the ideal state where the probability of generating a response is proportional to its reward. $Z(x)$ is the Partition Function (normalization constant) to ensure probabilities sum to 1.</p> \[\mathcal{L}(\pi, \lambda) = \underbrace{\sum_y \pi(y|x) \left[ r(x, y) - \beta \log \frac{\pi(y|x)}{\pi_{ref}(y|x)} \right]}_{\text{objective: reward + KL constraint}} - \underbrace{\lambda \left( \sum_y \pi(y|x) - 1 \right)}_{\text{constraint: sum to 1}}\] \[\frac{\partial \mathcal{L}}{\partial \pi(y|x)} = r(x, y) - \beta \left( \log \frac{\pi(y|x)}{\pi_{ref}(y|x)} + 1 \right) - \lambda = 0\] \[\pi(y|x) = \pi_{ref}(y|x) \exp\left( \frac{r(x, y)}{\beta} \right) \cdot \underbrace{\exp\left( - \frac{\beta + \lambda}{\beta} \right)}_{\text{normalization constant}}\] \[Z(x) = \sum_y \pi_{ref}(y|x) \exp\left( \frac{r(x, y)}{\beta} \right)\] \[\pi^*(y|x) = \frac{1}{Z(x)} \pi_{ref}(y|x) \exp\left( \frac{r(x, y)}{\beta} \right)\] <h5 id="4-log-space-transformation">4. Log-Space Transformation</h5> <p>we take the logarithm of the optimal policy $\pi^<em>$. This allows us to express the reward $r(x,y)$ and reference model $\pi_{ref}$ in terms of $\pi^</em>$ and the constant $Z(x)$. By rearranging the terms, we isolate the part that matches our factored objective function.</p> \[\log \pi^*(y|x) = \log \pi_{ref}(y|x) + \frac{1}{\beta} r(x, y) - \log Z(x)\] \[\log \pi_{ref}(y|x) + \frac{1}{\beta} r(x, y) = \log \pi^*(y|x) + \log Z(x)\] <h5 id="5-substitution">5. Substitution</h5> <p>we substitute the rearranged equation from Step 5 back into our objective function.</p> \[\begin{aligned} J(\pi) &amp;= -\beta \sum_y \pi(y|x) \left( \log \pi(y|x) - \left[ \log \pi^*(y|x) + \log Z(x) \right] \right) \\ &amp;= -\beta \sum_y \pi(y|x) \left( \log \frac{\pi(y|x)}{\pi^*(y|x)} - \log Z(x) \right) \end{aligned}\] <h5 id="6-simplification-to-kl-divergence">6. Simplification to KL Divergence:</h5> <p>we separate the terms to form the KL Divergence between our current policy $\pi$ and the optimal policy $\pi^*$. The second term simplifies because $\log Z(x)$ is constant with respect to $y$.</p> \[J(\pi) = -\beta \underbrace{\sum_y \pi(y|x) \log \frac{\pi(y|x)}{\pi^*(y|x)}}_{D_{KL}(\pi || \pi^*)} + \beta \sum_y \pi(y|x) \log Z(x)\] \[J(\pi) = -\beta D_{KL}(\pi(\cdot|x) || \pi^*(\cdot|x)) + \beta \log Z(x)\] <h5 id="7-the-optimization-equivalence">7. The Optimization Equivalence:</h5> <p>Since $\beta \log Z(x)$ is a constant (it depends only on the reward function and reference model, not the policy $\pi$ we are training), maximizing the original objective $J(\pi)$ is mathematically equivalent to minimizing the KL divergence between our policy and the optimal policy.</p> \[\max_{\pi} J(\pi) \iff \min_{\pi} D_{KL}(\pi || \pi^*)\] <h4 id="direct-preference-optimization-dpo">Direct Preference Optimization (DPO)</h4> <p>DPO re-parameterizes the reward function $r(x,y)$ using the optimal policy equation. Instead of training a separate reward model to predict which response is better, DPO directly optimizes the policy using a binary cross-entropy loss on preference pairs. It effectively treats the language model itself as the reward model.</p> \[\log \pi^*(y|x) = \log \pi_{ref}(y|x) + \frac{1}{\beta} r(x, y) - \log Z(x)\] \[\frac{1}{\beta} r(x, y) = \log \frac{\pi^*(y|x)}{\pi_{ref}(y|x)} + \log Z(x)\] \[r(x, y) = \beta \log \frac{\pi^*(y|x)}{\pi_{ref}(y|x)} + \beta \log Z(x)\] \[P(y_w \succ y_l | x) = \sigma(r(x, y_w) - r(x, y_l))\] \[\begin{aligned} r(x, y_w) - r(x, y_l) &amp;= \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} + \beta \log Z(x) \right) - \left( \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} + \beta \log Z(x) \right) \\ &amp;= \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} \end{aligned}\] \[P_{\theta}(y_w \succ y_l | x) = \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)} \right)\] \[\mathcal{L}_{\text{DPO}} = - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right]\] <ul> <li> <p><strong>Pros:</strong> <code class="language-plaintext highlighter-rouge">Removes the extensive resource need</code> for a separate Reward Model and the complex “Actor-Critic” loop of PPO. It is essentially supervised fine-tuning on preference pairs. It is also a supervised objective, which <code class="language-plaintext highlighter-rouge">avoids the high variance and instability often found in RL training</code>.</p> </li> <li> <p><strong>Cons:</strong> DPO learns strictly from the static preference dataset. Unlike PPO, it does not generate new samples during training to explore the solution space, which can lead to <code class="language-plaintext highlighter-rouge">distribution shift</code>. It can be <code class="language-plaintext highlighter-rouge">highly sensitive to the distribution and quality of the preference data</code>.</p> </li> </ul> <h4 id="identity-policy-optimization-ipo">Identity Policy Optimization (IPO)</h4> <p>IPO was introduced to address a theoretical flaw in DPO: the DPO loss can sometimes be minimized by driving the probability ratios (between policy and reference) to infinity, effectively ignoring the KL-divergence constraint. IPO fixes this by placing a quadratic regularization term directly on the “log-likelihood gap” between the winning and losing responses.</p> \[\mathcal{L}_{\text{IPO}} = \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[ \left( \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} - \frac{\tau}{2} \right)^2 \right]\] <ul> <li> <p><strong>Pros:</strong> IPO provides a <code class="language-plaintext highlighter-rouge">theoretically stricter bound</code> on the policy, preventing the model from drifting too far from the reference model (overfitting) even if the reward signal is strong. It also often yields <code class="language-plaintext highlighter-rouge">more stable convergence behavior</code> than DPO on noisy datasets.</p> </li> <li> <p><strong>Cons:</strong> The strict regularization can sometimes result in <code class="language-plaintext highlighter-rouge">slower adaptation</code> to the preference signal compared to the more aggressive DPO.</p> </li> </ul> <h4 id="kahneman-tversky-optimization-kto">Kahneman-Tversky Optimization (KTO)</h4> <p>Standard alignment methods (DPO, PPO) require paired data (Winner vs. Loser). KTO is inspired by <em>“Prospect Theory”</em> from behavioral economics. It eliminates the need for pairs entirely, optimizing based on whether a single sample is <em>Good</em> (thumbs up) or <em>Bad</em> (thumbs down) relative to the reference model.</p> \[\mathcal{L}_{\text{KTO}} = \underbrace{\sum_{y \in \text{Good}} w_{\text{good}} \cdot \left( 1 - \sigma(r_\theta(x,y) - z_{\text{ref}}) \right)}_{\text{Increase Good Cases}} + \underbrace{\sum_{y \in \text{Bad}} w_{\text{bad}} \cdot \left( 1 - \sigma(z_{\text{ref}} - r_\theta(x,y)) \right)}_{\text{Decrease Bad Cases}}\] <ul> <li><strong>Pros:</strong> Unlocks the use of vast amounts of <code class="language-plaintext highlighter-rouge">unpaired data</code> (e.g., customer support logs, star ratings) where explicit A/B comparisons are not available. Surprisingly, KTO often matches or exceeds DPO performance even without using paired preference data.</li> <li><strong>Cons:</strong> It introduces <code class="language-plaintext highlighter-rouge">weighting hyperparameters</code> ($w_{good}$, $w_{bad}$) that need to be tuned to balance the learning signal from positive and negative examples.</li> </ul> <h3 id="chapter-iv-make-rl-great-again">Chapter IV: Make RL Great Again</h3> <p>As models scale, the memory cost of PPO’s Critic model becomes prohibitive. Furthermore, for reasoning tasks, relative correctness within a group of generated outputs is often a stronger signal than a singular reward score.</p> <h4 id="group-relative-policy-optimization-grpo">Group Relative Policy Optimization (GRPO)</h4> <p>GRPO eliminates the need for a Value Function (Critic) entirely. Instead of using a learned Critic to estimate the baseline for the advantage function, GRPO samples a group of outputs ${y_1, y_2, \dots, y_G}$ for the same prompt $x$ and uses the group average reward as the baseline.Essentially, it asks: “How good is this specific response compared to the other variants I just generated?”</p> \[\mathcal{L}_{\text{GRPO}} = \mathcal{L}_{\text{surrogate}} + \beta D_{\text{KL}}(\pi_\theta || \pi_{\text{ref}})\] \[\mathcal{L}_{\text{surrogate}} = - \frac{1}{G} \sum_{i=1}^{G} \left[ \min \left( \rho_i A_i, \text{clip}(\rho_i, 1-\epsilon, 1+\epsilon) A_i \right) \right]\] \[A_i = \frac{r_i - \text{mean}(\{r_1 \dots r_G\})}{\text{std}(\{r_1 \dots r_G\})}\] <ul> <li><strong>Pros:</strong> <ul> <li><strong>Memory Efficiency:</strong> By removing the Critic model, it significantly reduces VRAM usage (often saving ~50% of the memory required for PPO), allowing for training larger models or using larger batch sizes.</li> <li><strong>Reasoning Power:</strong> It has proven exceptionally effective for “Aha! moment” domains (Math, Code, Logic) where verifying a solution is easier than predicting its value. The group comparison stabilizes the gradient in these sparse-reward environments.</li> <li><strong>Simplicity:</strong> Fewer moving parts than PPO makes it easier to implement and tune.</li> </ul> </li> <li><strong>Cons:</strong> <ul> <li><strong>Generation Cost:</strong> The training loop requires generating multiple outputs ($G$) for every single prompt, which increases the computational cost of the data collection phase.</li> <li><strong>Dependency on Reward Function:</strong> Since it relies heavily on relative ranking within a group, it requires a robust reward signal (like a unit test or a verifier) to be effective. It may be less stable for purely subjective tasks where “better than average” is ambiguous.</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="academia"/><category term="math"/><category term="code"/><summary type="html"><![CDATA[Preference Alignment Preliminary Notes]]></summary></entry><entry><title type="html">Challenges in Code Generation</title><link href="https://elfsong.github.io/blog/2025/code-challenge/" rel="alternate" type="text/html" title="Challenges in Code Generation"/><published>2025-10-10T00:00:00+00:00</published><updated>2025-10-10T00:00:00+00:00</updated><id>https://elfsong.github.io/blog/2025/code-challenge</id><content type="html" xml:base="https://elfsong.github.io/blog/2025/code-challenge/"><![CDATA[<h2 id="training-data">Training Data</h2> <ul> <li>Training <ul> <li>Generalization</li> <li>Preference</li> </ul> </li> <li>Inference <ul> <li>Memory Management</li> <li>Hallucination</li> <li>Integration</li> </ul> </li> <li>Evaluation <ul> <li>Benchmarks</li> <li>Metrics</li> </ul> </li> </ul>]]></content><author><name>Mingzhe Du</name></author><category term="academia"/><category term="math"/><category term="code"/><summary type="html"><![CDATA[Challenges in Code Generation]]></summary></entry><entry><title type="html">PREDICTING AND OPTIMIZING LLVM COMPILER PASS ORDER</title><link href="https://elfsong.github.io/blog/2025/llvm/" rel="alternate" type="text/html" title="PREDICTING AND OPTIMIZING LLVM COMPILER PASS ORDER"/><published>2025-06-08T00:00:00+00:00</published><updated>2025-06-08T00:00:00+00:00</updated><id>https://elfsong.github.io/blog/2025/llvm</id><content type="html" xml:base="https://elfsong.github.io/blog/2025/llvm/"><![CDATA[<div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/llvm.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div>]]></content><author><name></name></author><category term="research"/><category term="llvm"/><summary type="html"><![CDATA[Predicting and optimizing LLVM compiler pass order]]></summary></entry><entry><title type="html">Where the Time Comes From?</title><link href="https://elfsong.github.io/blog/2025/time/" rel="alternate" type="text/html" title="Where the Time Comes From?"/><published>2025-06-06T00:00:00+00:00</published><updated>2025-06-06T00:00:00+00:00</updated><id>https://elfsong.github.io/blog/2025/time</id><content type="html" xml:base="https://elfsong.github.io/blog/2025/time/"><![CDATA[<p>When you write code that deals with time, you might think getting the current time is simple. But beneath the surface, there are two different kinds of “clocks” your system uses, and choosing the wrong one can lead to mysterious bugs and incorrect measurements. Let’s break down the difference between a <strong>Wall Clock</strong> and a <strong>Monotonic Clock</strong>.</p> <p>Imagine you need to time a 100-meter sprint. You have two options:</p> <ul> <li>Look at <em>the big clock on the stadium</em> wall when the race starts and when it finishes.</li> <li>Use a <em>stopwatch</em> in your hand.</li> </ul> <p>Which one would you trust for an accurate result? Almost certainly the stopwatch. This simple analogy is the key to understanding the two clocks in your computer.</p> <h2 id="wall-clock">Wall Clock</h2> <p>A Wall Clock (or system_clock) tells you the current, human-readable date and time. It’s what you’d use to answer the question, <em>“What time is it right now?”</em> Its goal is to <strong>be accurate according to the real world.</strong></p> <h3 id="when-to-use-wall-clock">When to use Wall Clock</h3> <ul> <li>Generating timestamps for logs (Event happened at 2025-06-07 13:55:00).</li> <li>Scheduling future events (Run this task next Monday at 9:00 AM).</li> <li>Displaying dates and times to a user.</li> </ul> <h3 id="the-problem">The Problem</h3> <p>Wall Clocks are not reliable for measuring time intervals. Because they can suddenly jump forward or even go backward. This can happen for several reasons:</p> <ul> <li><strong>NTP Sync</strong>: Your computer periodically syncs with a Network Time Protocol server to correct its time.</li> <li><strong>Daylight Saving Time</strong>: The clock is adjusted forward or backward by an hour.</li> <li><strong>Manual Changes</strong>: A user or system administrator can change the time manually.</li> </ul> <p>Measuring a negative duration is impossible, but with a Wall Clock, your code might report it. This is why you should never use a Wall Clock for performance benchmarks or calculating timeouts.</p> <h2 id="monotonic-clock">Monotonic Clock</h2> <p>A Monotonic Clock (<em>steady_clock</em> or <em>perf_counter</em>) is designed for one purpose: to <strong>measure elapsed time intervals accurately</strong>. Its defining feature is that it only ever moves forward.</p> <p>Think of it as a stopwatch that starts counting from zero when your system boots up. The absolute value it gives you is meaningless (it’s just a large number of nanoseconds since booting the system), but the difference between two readings is incredibly precise and reliable. It isn’t affected by NTP syncs, Daylight Saving Time, or any other adjustments to the system’s Wall Clock.</p> <h3 id="when-to-use-monotonic-clock">When to use Monotonic Clock</h3> <ul> <li>Benchmarking how long a piece of code takes to run.</li> <li>Implementing timeouts for network requests or operations.</li> <li>Controlling animations or physics in a game engine.</li> </ul>]]></content><author><name>Mingzhe Du</name></author><category term="academia"/><category term="math"/><category term="code"/><summary type="html"><![CDATA[Wall Clocks vs. Monotonic Clocks]]></summary></entry><entry><title type="html">Afterburner</title><link href="https://elfsong.github.io/blog/2025/afterburner/" rel="alternate" type="text/html" title="Afterburner"/><published>2025-05-26T00:00:00+00:00</published><updated>2025-05-26T00:00:00+00:00</updated><id>https://elfsong.github.io/blog/2025/afterburner</id><content type="html" xml:base="https://elfsong.github.io/blog/2025/afterburner/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>Large Language Models (LLMs) generate functionally correct solutions but often fall short in code efficiency, a critical bottleneck for real-world deployment. In this paper, we introduce a novel test-time iterative optimization framework to address this, employing a closed-loop system where LLMs iteratively refine code based on empirical performance feedback from an execution sandbox. We explore three training strategies: Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Group Relative Policy Optimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark show that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO, using reinforcement learning (RL) with execution feedback, continuously optimizes code performance, significantly boosting both pass@1 (from 47% to 62%) and the likelihood of outperforming human submissions in efficiency (from 31% to 45%). Our work demonstrates effective test-time code efficiency improvement and critically reveals the power of RL in teaching LLMs to truly self-improve code efficiency.</p> <h2 id="venus-code-efficiency-dataset">Venus: Code Efficiency Dataset</h2> <h2 id="monolith-code-execution-sandbox">Monolith: Code Execution Sandbox</h2> <h2 id="afterburner-test-time-iterative-optimization-framework">Afterburner: Test-Time Iterative Optimization Framework</h2> <h2 id="experiments">Experiments</h2> <pre><code class="language-echarts">{
    "title": {
        "text": "Pass@1 Over Iterations"
    },
    "responsive": true,
    "tooltip": {
        "trigger": "axis"
    },
    "legend": {
        "top": "50px",
        "data": ["Base Model", "Afterburner 3B-SFT", "Afterburner 3B-DPO", "Afterburner 3B-GRPO"]
    },
    "grid": {
        "left": "3%",
        "right": "4%",
        "bottom": "3%",
        "containLabel": true
    },
    "toolbox": {
        "feature": {
        "saveAsImage": {}
        }
    },
    "xAxis": {
        "type": "category",
        "boundaryGap": false,
        "data": ["Iter 0", "Iter 1", "Iter 2", "Iter 3", "Iter 4", "Iter 5", "Iter 6", "Iter 7", "Iter 8", "Iter 9", "Iter 10"]
    },
    "yAxis": {
        "type": "value",
        "name": "Pass@1"
    },
    "series": [
        {
        "name": "Base Model",
        "type": "line",
        "data": [27.99, 28.33, 28.67, 28.67, 29.00, 29.08, 29.17, 29.24, 29.33, 29.33, 29.33]
        },
        {
        "name": "Afterburner 3B-SFT",
        "type": "line",
        "data": [46.00, 46.00, 46.33, 47.00, 48.33, 48.33, 48.67, 48.67, 48.67, 48.67, 48.67]
        },
        {
        "name": "Afterburner 3B-DPO",
        "type": "line",
        "data": [43.00, 50.00, 51.33, 51.50, 51.67, 51.67, 51.67, 51.67, 51.67, 51.67, 51.67]
        },
        {
        "name": "Afterburner 3B-GRPO",
        "type": "line",
        "data": [47.33, 50.33, 52.00, 54.50, 57.00, 58.17, 59.34, 60.50, 61.18, 61.67, 61.67]
        }
    ]
}
</code></pre> <pre><code class="language-echarts">{
    "title": {
        "text": "Beyond-I Over Iterations"
    },
    "responsive": true,
    "tooltip": {
        "trigger": "axis"
    },
    "legend": {
        "top": "50px",
        "data": ["Base Model", "Afterburner 3B-SFT", "Afterburner 3B-DPO", "Afterburner 3B-GRPO"]
    },
    "grid": {
        "left": "3%",
        "right": "4%",
        "bottom": "3%",
        "containLabel": true
    },
    "toolbox": {
        "feature": {
        "saveAsImage": {}
        }
    },
    "xAxis": {
        "type": "category",
        "boundaryGap": false,
        "data": ["Iter 0", "Iter 1", "Iter 2", "Iter 3", "Iter 4", "Iter 5", "Iter 6", "Iter 7", "Iter 8", "Iter 9", "Iter 10"]
    },
    "yAxis": {
        "type": "value",
        "name": "Beyond-I"
    },
    "series": [
        {
        "name": "Base Model",
        "type": "line",
        "data": [10.29, 10.77, 11.25, 11.49, 11.72, 11.83, 11.96, 12.07, 12.07, 12.07, 12.07]
        },
        {
        "name": "Afterburner 3B-SFT",
        "type": "line",
        "data": [21.01, 21.09, 21.50, 21.88, 22.25, 22.31, 22.38, 22.44, 22.50, 22.50, 22.50]
        },
        {
        "name": "Afterburner 3B-DPO",
        "type": "line",
        "data": [19.13, 26.25, 27.05, 27.42, 27.95, 28.11, 28.51, 29.51, 29.51, 29.51, 29.51]
        },
        {
        "name": "Afterburner 3B-GRPO",
        "type": "line",
        "data": [18.24, 24.81, 29.44, 30.85, 33.56, 35.48, 37.09, 38.01, 38.62, 38.95, 38.95]
        }
    ]
}
</code></pre>]]></content><author><name>Mingzhe Du</name></author><category term="academia"/><category term="math"/><category term="code"/><summary type="html"><![CDATA[By Humans, Beyond Humans.]]></summary></entry><entry><title type="html">Advancements in Automated MCQ Generation</title><link href="https://elfsong.github.io/blog/2025/mcq/" rel="alternate" type="text/html" title="Advancements in Automated MCQ Generation"/><published>2025-05-26T00:00:00+00:00</published><updated>2025-05-26T00:00:00+00:00</updated><id>https://elfsong.github.io/blog/2025/mcq</id><content type="html" xml:base="https://elfsong.github.io/blog/2025/mcq/"><![CDATA[]]></content><author><name></name></author><category term="academia"/><category term="math"/><category term="code"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">LaTeX Color Palette</title><link href="https://elfsong.github.io/blog/2025/Xcolor/" rel="alternate" type="text/html" title="LaTeX Color Palette"/><published>2025-05-01T15:12:00+00:00</published><updated>2025-05-01T15:12:00+00:00</updated><id>https://elfsong.github.io/blog/2025/Xcolor</id><content type="html" xml:base="https://elfsong.github.io/blog/2025/Xcolor/"><![CDATA[<iframe src="/assets/pdf/Xcolor.pdf" width="100%" height="600" style="border: none;"></iframe>]]></content><author><name></name></author><category term="academia"/><category term="math"/><summary type="html"><![CDATA[Latex Color]]></summary></entry><entry><title type="html">NTU × NUS Exchange</title><link href="https://elfsong.github.io/blog/2025/event/" rel="alternate" type="text/html" title="NTU × NUS Exchange"/><published>2025-04-17T21:01:00+00:00</published><updated>2025-04-17T21:01:00+00:00</updated><id>https://elfsong.github.io/blog/2025/event</id><content type="html" xml:base="https://elfsong.github.io/blog/2025/event/"><![CDATA[<iframe src="https://lucky-reactor.notion.site/ebd/1d933f409b0480c28717e0b0b66ef48a" width="100%" height="1000" frameborder="0" allowfullscreen=""/>]]></content><author><name></name></author><category term="academia"/><category term="event"/><summary type="html"><![CDATA[Submit new event to NTU × NUS Exchange]]></summary></entry><entry><title type="html">How to execute LLVM IR generated from Codon?</title><link href="https://elfsong.github.io/blog/2024/Codon/" rel="alternate" type="text/html" title="How to execute LLVM IR generated from Codon?"/><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://elfsong.github.io/blog/2024/Codon</id><content type="html" xml:base="https://elfsong.github.io/blog/2024/Codon/"><![CDATA[<p>1) Assume we have a python snippet like that:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># File Name: foo.py
</span><span class="k">def</span> <span class="nf">foo</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Hello World!</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">foo</span><span class="p">()</span>
</code></pre></div></div> <p>2) Convert the code to LLVM IR using Codon:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>codon build <span class="nt">-release</span> <span class="nt">-llvm</span> foo.py
</code></pre></div></div> <p>3) Compile Codon runtime (you need to install and setup CMake in advance):</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Clone Codon Repo</span>
git clone https://github.com/exaloop/codon.git

<span class="c"># Create a build dir</span>
<span class="nb">mkdir </span>build
<span class="nb">cd </span>build

<span class="c"># Compile the Codon runtime</span>
cmake ..
make
</code></pre></div></div> <p>4) Execute the generated IR using ‘lli’ with Codon runtime libcodonrt.so):</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>lli <span class="nt">-load</span> ./codon/build/libcodonrt.so ./foo.ll 
</code></pre></div></div>]]></content><author><name></name></author><category term="thinktank"/><summary type="html"><![CDATA[Python with native machine speed]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://elfsong.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://elfsong.github.io/blog/2024/tabs</id><content type="html" xml:base="https://elfsong.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="040b4767-50dd-4f49-a515-e2ba29644879" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="040b4767-50dd-4f49-a515-e2ba29644879" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="bea538a1-146c-4690-96ef-9623794701ff" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="bea538a1-146c-4690-96ef-9623794701ff" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="54290c47-1025-4c94-8b09-11734dd31cc5" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="54290c47-1025-4c94-8b09-11734dd31cc5" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry></feed>